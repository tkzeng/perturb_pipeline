# =============================================================================
# DOWNSTREAM ANALYSIS PIPELINE
# =============================================================================
# This file contains rules for downstream analysis after initial processing
# including sublibrary combination, standard analyses, and UMAP generation
#
# Prerequisites: Run preprocessing.smk first to generate QC metrics and cell calling
#
# To run: snakemake -s downstream.smk --configfile config.yaml

import os
import sys
from datetime import datetime
from pathlib import Path
from scripts.pipeline_utils import get_guide_gex_pairings
from scripts.snakemake_helpers import (
    # Path helper functions
    get_scratch_path, get_results_path, get_logs_path,
    get_reference_path, get_raw_data_path, print_path_configuration,
    # Sample management functions
    load_sample_info, get_sample_ids, get_samples_by_type,
    extract_pool, extract_sample, get_sample_pool,
    # File finding functions
    find_fastq_file, print_sample_summary,
    # Output generation functions
    get_downstream_outputs
)

# Define base paths using helper functions
SCRATCH = get_scratch_path(config=config)
RESULTS = get_results_path(config=config)
LOGS = get_logs_path(config=config)
REFERENCES = get_reference_path(config=config)

# Load sample information and IDs
IDS = get_sample_ids(config)
GEX_IDS = get_samples_by_type('gex', config)
GUIDE_IDS = get_samples_by_type('guide', config)
# Get guide-GEX pairings from the centralized function
guide_to_gex, gex_to_guide = get_guide_gex_pairings(config['sample_info_file'])

# Wrapper function for load_sample_info  
def load_sample_info_wrapper():
    return load_sample_info(config)

rule all:
    """Target rule for downstream analysis pipeline with complete report generation"""
    input:
        # Final complete report - this triggers all downstream analyses via get_downstream_outputs()
        final_report=f"{RESULTS}/downstream_report/COMPLETE_REPORT_DONE.txt"


# =============================================================================
# SUBLIBRARY COMBINATION
# =============================================================================
rule combine_sublibraries:
    """Combine cell-called sublibraries into a single analysis-ready file"""
    input:
        # Annotated h5ad files (have annotations but need cell calling applied)
        h5ad_files=lambda wildcards: [
            f"{SCRATCH}/{row['sample_id']}/kb_all_{wildcards.source}_{wildcards.processing}/counts_filtered/adata.h5ad"
            for _, row in load_sample_info(config).iterrows()
            if row['sample_type'] == 'gex'
        ],
        # Cell barcode files for the chosen method
        barcode_files=lambda wildcards: [
            f"{RESULTS}/preprocessing_report/data/per_sample/{wildcards.source}_{wildcards.processing}/{row['sample_id']}/cell_calling/{row['sample_id']}_{config['cell_calling']['default_method']}_cell_barcodes.txt"
            for _, row in load_sample_info(config).iterrows()
            if row['sample_type'] == 'gex'
        ],
        # Per-sample QC cell list files (generated by QC metrics rules)
        filter_files=lambda wildcards: [
            f"{RESULTS}/preprocessing_report/data/per_sample/{wildcards.source}_{wildcards.processing}/{row['sample_id']}/qc_cell_lists.tsv"
            for _, row in load_sample_info(config).iterrows()
            if row['sample_type'] == 'gex'
        ],
        # Per-sample GMM threshold tables
        threshold_tables=lambda wildcards: [
            f"{RESULTS}/preprocessing_report/data/per_sample/{wildcards.source}_{wildcards.processing}/{row['sample_id']}/qc_metrics/gmm_thresholds_per_cell.tsv"
            for _, row in load_sample_info(config).iterrows()
            if row['sample_type'] == 'gex'
        ],
        script="scripts/combine_sublibraries.py"
    output:
        combined=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/combined_{{source}}_{{processing}}.h5ad"
    params:
        cell_calling_method=config['cell_calling']['default_method'],
        config_file=workflow.configfiles[0]
    log:
        f"{LOGS}/combine_sublibraries_{{source}}_{{processing}}.log"
    wildcard_constraints:
        source="main|undetermined|all",
        processing="raw|recovered|merged"
    threads: 4
    resources:
        mem_mb=config["resources"]["analysis"]["mem_mb"]
    shell:
        """
        python {input.script} \
            --h5ad-files {input.h5ad_files} \
            --barcode-files {input.barcode_files} \
            --filter-files {input.filter_files} \
            --threshold-tables {input.threshold_tables} \
            --cell-calling-method {params.cell_calling_method} \
            --config {params.config_file} \
            --output {output.combined} \
            &> {log}
        """


# =============================================================================
# PERTURBATION PREPROCESSING AND ANALYSIS  
# =============================================================================
rule standard_analyses:
    """Standard analyses including normalization, HVG selection, clustering, UMAP, 
    cell cycle scoring, differential expression, and gene scoring.
    """
    input:
        combined=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/combined_{{source}}_{{processing}}.h5ad",
        script="scripts/standard_analyses.py"
    output:
        preprocessed=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/preprocessed_{{source}}_{{processing}}.h5ad"
    params:
        outdir=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/standard_analyses_{{source}}_{{processing}}",
        config_file=workflow.configfiles[0]
    wildcard_constraints:
        source="main|undetermined|all",
        processing="raw|recovered|merged"
    log:
        f"{LOGS}/standard_analyses_{{source}}_{{processing}}.log"
    threads: 4
    resources:
        mem_mb=config["resources"]["analysis"]["mem_mb"]
    shell:
        """
        mkdir -p {params.outdir}
        
        python {input.script} \
            --config-file {params.config_file} \
            --input-file {input.combined} \
            --outdir {params.outdir} \
            --final-output-file {output.preprocessed} \
            &> {log}
        """


rule plot_standard_analyses_umap:
    """Generate UMAP plots from preprocessed standard_analyses output.
    
    Creates individual PNG files for dashboard integration with visualizations colored by:
    - QC metrics (mt%, ribo%, total counts, etc.)
    - Cell cycle scores
    - Target gene categories and DE scores
    - Guide MOI categories
    - Leiden clustering results
    """
    input:
        preprocessed=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/preprocessed_{{source}}_{{processing}}.h5ad",
        script="scripts/plot_standard_analyses_umap.py"
    output:
        plot_dir=directory(f"{RESULTS}/downstream_report/plots/umap/{{source}}_{{processing}}"),
        complete=f"{RESULTS}/downstream_report/plots/umap/{{source}}_{{processing}}.complete"
    params:
        outdir=f"{RESULTS}/downstream_report/plots/umap/{{source}}_{{processing}}"
    wildcard_constraints:
        source="main|undetermined|all",
        processing="raw|recovered|merged"
    log:
        f"{LOGS}/plot_standard_analyses_umap_{{source}}_{{processing}}.log"
    threads: 1
    resources:
        mem_mb=16000  # 16GB for plotting
    shell:
        """
        python {input.script} \
            --input-file {input.preprocessed} \
            --output-dir {params.outdir} &> {log}
        """


# =============================================================================
# PSEUDOBULK ANALYSIS WITH TMM NORMALIZATION
# =============================================================================
rule pseudobulk_tmm:
    """Generate pseudobulk expression profiles with TMM normalization
    
    WARNING: This rule is UNTESTED and may need adjustments for your data.
    
    Aggregates single-cell expression by specified grouping variables
    (e.g., biological_sample, perturbation) and applies TMM normalization
    using edgeR for compositional bias correction.
    
    Outputs:
    - Raw pseudobulk counts
    - TMM-normalized CPM
    - Simple CPM (without normalization factors)
    - Group and gene metadata
    """
    input:
        preprocessed=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/preprocessed_{{source}}_{{processing}}.h5ad",
        combined=f"{RESULTS}/{config['combined_sublibraries']['output_dir']}/combined_{{source}}_{{processing}}.h5ad",
        script="scripts/pseudobulk_tmm_normalization.py",
        r_script="scripts/run_edger_tmm.R"
    output:
        tmm_cpm=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_tmm_cpm.tsv",
        simple_cpm=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_simple_cpm.tsv",
        raw_counts=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_raw_counts.tsv",
        group_metadata=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_group_metadata.tsv",
        gene_metadata=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_gene_metadata.tsv"
    params:
        output_dir=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}",
        groupby_cols=lambda wildcards: ' '.join(config['pseudobulk']['groupby_cols']),
        covariate_cols=lambda wildcards: ' '.join(config['pseudobulk']['covariate_cols']),
        output_prefix="pseudobulk"
    wildcard_constraints:
        source="main|undetermined|all",
        processing="raw|recovered|merged"
    log:
        f"{LOGS}/pseudobulk_tmm_{{source}}_{{processing}}.log"
    threads: 2
    resources:
        mem_mb=config["resources"]["analysis"]["mem_mb"]
    shell:
        """
        mkdir -p {params.output_dir}
        
        # Load R module for edgeR
        ml R/4.4 2>/dev/null || echo "R module not loaded - using system R"
        
        # Run pseudobulk analysis
        python {input.script} \
            --input-file {input.preprocessed} \
            --raw-file {input.combined} \
            --output-dir {params.output_dir} \
            --groupby-cols {params.groupby_cols} \
            --covariate-cols {params.covariate_cols} \
            --output-prefix {params.output_prefix} \
            &> {log}
        
        # Unload R module
        ml purge 2>/dev/null || true
        
        echo "Pseudobulk analysis complete. Results in {params.output_dir}" | tee -a {log}
        """


# =============================================================================
# DIFFERENTIAL EXPRESSION ANALYSIS
# =============================================================================
rule differential_expression:
    """Perform differential expression analysis using edgeR on pseudobulk data
    
    WARNING: This rule is UNTESTED and may need adjustments for your data.
    
    Takes the TMM-normalized pseudobulk counts and performs pairwise DE analysis
    for specified contrasts. Generates comprehensive results tables and 
    publication-ready visualizations including volcano plots and MA plots.
    
    Outputs are organized following the existing pipeline structure with data
    and plots subdirectories.
    """
    input:
        raw_counts=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_raw_counts.tsv",
        norm_factors=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_norm_factors.tsv",
        group_metadata=f"{RESULTS}/pseudobulk/{{source}}_{{processing}}/pseudobulk_group_metadata.tsv",
        script="scripts/run_edger_de_with_plots.R"
    output:
        complete=f"{RESULTS}/differential_expression/{{source}}_{{processing}}/analysis.complete"
    params:
        output_dir=f"{RESULTS}/differential_expression/{{source}}_{{processing}}",
        contrasts=lambda wildcards: ','.join(config['differential_expression']['contrasts']),
        fdr_threshold=lambda wildcards: config['differential_expression']['fdr_threshold'],
        logfc_threshold=lambda wildcards: config['differential_expression']['logfc_threshold'],
        group_column=lambda wildcards: config['differential_expression']['group_column'],
        design_formula=lambda wildcards: config['differential_expression']['design_formula'],
        plot_dir=f"{RESULTS}/downstream_report/plots"
    wildcard_constraints:
        source="main|undetermined|all",
        processing="raw|recovered|merged"
    log:
        f"{LOGS}/differential_expression_{{source}}_{{processing}}.log"
    threads: 2
    resources:
        mem_mb=16000  # 16GB for DE analysis and plotting
    shell:
        """
        # Load R module
        ml R/4.4 2>/dev/null || echo "R module not loaded - using system R"
        
        # Run differential expression analysis with plots
        Rscript {input.script} \
            --counts {input.raw_counts} \
            --norm-factors {input.norm_factors} \
            --metadata {input.group_metadata} \
            --output-dir {params.output_dir} \
            --contrasts {params.contrasts} \
            --fdr-threshold {params.fdr_threshold} \
            --logfc-threshold {params.logfc_threshold} \
            --group-column {params.group_column} \
            --design-formula "{params.design_formula}" \
            --plot-dir {params.plot_dir} \
            --source {wildcards.source} \
            --processing {wildcards.processing} \
            &> {log}
        
        # Create completion marker
        touch {output.complete}
        
        # Unload R module
        ml purge 2>/dev/null || true
        
        echo "Differential expression analysis complete. Results in {params.output_dir}" | tee -a {log}
        """


# =============================================================================
# FINAL REPORT GENERATION
# =============================================================================
rule generate_downstream_report:
    """Package UMAP plots from downstream analyses for laptop viewing with Streamlit dashboard
    
    This generates a dashboard specifically for downstream analysis outputs:
    - UMAP visualizations from standard analyses
    - Clustering results
    - Differential expression scores
    
    Note: QC metrics are packaged separately by preprocessing.smk
    Run this after all downstream analyses are complete.
    """
    input:
        # Get all downstream outputs (primarily UMAP plots)
        files=get_downstream_outputs(config, report_dir="downstream_report"),
        sample_info=config['sample_info_file'],
        dashboard_script="scripts/streamlit_qc_dashboard_optimized.py",
        package_script="scripts/package_qc_for_laptop_fast_no_json.py"
    output:
        done=f"{RESULTS}/downstream_report/COMPLETE_REPORT_DONE.txt"
    params:
        timestamp=lambda wildcards: datetime.now().strftime("%Y%m%d_%H%M%S")
    log:
        f"{LOGS}/generate_downstream_report.log"
    shell:
        """
        echo "Generating downstream analysis dashboard (UMAP plots)..." | tee {log}
        
        # Run packaging script with 'complete' suffix to distinguish from basic report
        python -u {input.package_script} \
            --qc-report-dir {RESULTS}/downstream_report \
            --sample-info {input.sample_info} \
            --dashboard-script {input.dashboard_script} \
            --output-archive {RESULTS}/qc_dashboard_downstream_{params.timestamp}.tar.gz \
            --per-cell-method-filter {config[cell_calling][default_method]} \
            --guide-cutoff-filter 1,2 \
            --threads {threads} &>> {log}
        
        # Create done file
        touch {output.done}
        
        echo "Downstream dashboard generated: qc_dashboard_downstream_{params.timestamp}.tar.gz" | tee -a {log}
        """
